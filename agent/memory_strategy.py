from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Callable
from langchain_core.messages import BaseMessage, SystemMessage, AIMessage, HumanMessage, RemoveMessage
from langchain_core.messages.utils import trim_messages
from langchain_core.language_models import BaseLanguageModel
import json

# ç‰¹æ®Šå¸¸é‡ï¼Œç”¨äºç§»é™¤æ‰€æœ‰æ¶ˆæ¯
REMOVE_ALL_MESSAGES = "REMOVE_ALL_MESSAGES"

class BaseMemoryStrategy(ABC):
    """è®°å¿†ç­–ç•¥çš„åŸºç±»"""
    
    @abstractmethod
    def create_pre_model_hook(self) -> Callable:
        """åˆ›å»ºä¸€ä¸ªç”¨äº create_react_agent çš„ pre_model_hook å‡½æ•°"""
        pass

    @staticmethod
    def default_strategy() -> 'BaseMemoryStrategy':
        return NoOpStrategy()

class NoOpStrategy(BaseMemoryStrategy):
    """æ— æ“ä½œç­–ç•¥ - ä¸è¿›è¡Œä»»ä½•æ¶ˆæ¯ä¿®å‰ª"""
    
    def create_pre_model_hook(self) -> Callable:
        def pre_model_hook(state: Dict[str, Any]) -> Dict[str, Any]:
            messages = state.get("messages", [])
            print(f"ğŸ§  [æ— æ“ä½œ] ä¿ç•™å…¨éƒ¨ {len(messages)} æ¡æ¶ˆæ¯")
            return {"llm_input_messages": messages}
        
        return pre_model_hook

class SlidingWindowStrategy(BaseMemoryStrategy):
    """æ»‘åŠ¨çª—å£ç­–ç•¥ - åªä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯"""
    
    def __init__(self, max_messages: int = 10):
        self.max_messages = max_messages
        
    def create_pre_model_hook(self) -> Callable:
        def pre_model_hook(state: Dict[str, Any]) -> Dict[str, Any]:
            messages = state.get("messages", [])
            
            # åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå…¶ä»–æ¶ˆæ¯
            system_messages = [msg for msg in messages if isinstance(msg, SystemMessage)]
            other_messages = [msg for msg in messages if not isinstance(msg, SystemMessage)]
            
            # ä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯
            trimmed_other = other_messages[-self.max_messages:] if len(other_messages) > self.max_messages else other_messages
            
            trimmed_messages = system_messages + trimmed_other
            
            print(f"ğŸ§  [æ»‘åŠ¨çª—å£] æ¶ˆæ¯ä¿®å‰ª: {len(messages)} -> {len(trimmed_messages)}")
            
            # è¿”å›æ ¼å¼å¿…é¡»åŒ…å« llm_input_messages
            return {"llm_input_messages": trimmed_messages}
        
        return pre_model_hook

class TokenLimitStrategy(BaseMemoryStrategy):
    """Tokené™åˆ¶ç­–ç•¥ - åŸºäºtokenæ•°é‡é™åˆ¶æ¶ˆæ¯"""
    
    def __init__(self, max_tokens: int = 4096, strategy: str = "last", token_counter=None):
        self.max_tokens = max_tokens
        self.strategy = strategy
        self.token_counter = token_counter or self._default_token_counter
        
    def _default_token_counter(self, messages: List[BaseMessage]) -> int:
        """ç®€å•çš„tokenè®¡æ•°å™¨"""
        return sum(len(str(msg.content)) // 4 for msg in messages)  # ç²—ç•¥ä¼°è®¡
        
    def create_pre_model_hook(self) -> Callable:
        def pre_model_hook(state: Dict[str, Any]) -> Dict[str, Any]:
            messages = state.get("messages", [])
            
            # è°ƒç”¨langchainçš„trim_messageså‡½æ•°è¿›è¡Œä¿®å‰ª
            trimmed_messages = trim_messages(
                messages,
                strategy=self.strategy,
                token_counter=self.token_counter,
                max_tokens=self.max_tokens,
                start_on="human",
                end_on=("human", "tool"),
                include_system=True,
            )
            
            print(f"ğŸ§  [Tokené™åˆ¶] æ¶ˆæ¯ä¿®å‰ª: {len(messages)} -> {len(trimmed_messages)} (æœ€å¤§ {self.max_tokens} tokens)")
            
            return {"llm_input_messages": trimmed_messages}
        
        return pre_model_hook

class SummaryStrategy(BaseMemoryStrategy):
    """æ‘˜è¦ç­–ç•¥V2 - ä½¿ç”¨æ”¹è¿›çš„æ£€æŸ¥ç‚¹æœºåˆ¶"""
    
    def __init__(self, llm: BaseLanguageModel, keep_recent: int = 4, 
                 summary_max_tokens: int = 500, checkpoint_interval: int = 10):
        self.llm = llm
        self.keep_recent = keep_recent
        self.summary_max_tokens = summary_max_tokens
        self.checkpoint_interval = checkpoint_interval
        
        # å­˜å‚¨æ£€æŸ¥ç‚¹çš„ç´¯ç§¯æ‘˜è¦
        # key: æ¶ˆæ¯æ•°é‡, value: (ç´¯ç§¯æ‘˜è¦, è¯¥æ£€æŸ¥ç‚¹çš„åŸå§‹æ¶ˆæ¯æ•°)
        self._checkpoints: Dict[int, tuple[str, int]] = {}
        
    def create_pre_model_hook(self) -> Callable:
        def pre_model_hook(state: Dict[str, Any]) -> Dict[str, Any]:
            messages = state.get("messages", [])
            
            if len(messages) <= self.keep_recent + 1:
                return {"llm_input_messages": messages}
            
            system_messages = [msg for msg in messages if isinstance(msg, SystemMessage)]
            other_messages = [msg for msg in messages if not isinstance(msg, SystemMessage)]
            
            to_summarize = other_messages[:-self.keep_recent]
            recent_messages = other_messages[-self.keep_recent:]
            
            current_count = len(to_summarize)
            
            # æ‰¾åˆ°æœ€è¿‘çš„æ£€æŸ¥ç‚¹
            checkpoint_counts = sorted([k for k in self._checkpoints.keys() if k <= current_count])
            last_checkpoint_count = checkpoint_counts[-1] if checkpoint_counts else 0
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ›å»ºæ–°æ£€æŸ¥ç‚¹
            need_new_checkpoint = (
                current_count >= self.checkpoint_interval and
                (not checkpoint_counts or current_count - last_checkpoint_count >= self.checkpoint_interval)
            )
            
            if need_new_checkpoint:
                if last_checkpoint_count > 0:
                    # åŸºäºä¸Šä¸€ä¸ªæ£€æŸ¥ç‚¹åˆ›å»ºæ–°æ£€æŸ¥ç‚¹
                    last_summary, _ = self._checkpoints[last_checkpoint_count]
                    new_messages = to_summarize[last_checkpoint_count:current_count]
                    
                    new_text = "\n".join([
                        f"{msg.__class__.__name__}: {msg.content}" 
                        for msg in new_messages
                    ])
                    
                    prompt = f"""ä¹‹å‰çš„ç´¯ç§¯æ‘˜è¦ï¼ˆåŒ…å«å‰{last_checkpoint_count}æ¡æ¶ˆæ¯ï¼‰ï¼š
{last_summary}

æ–°å¢çš„å¯¹è¯å†…å®¹ï¼ˆç¬¬{last_checkpoint_count+1}åˆ°{current_count}æ¡ï¼‰ï¼š
{new_text}

è¯·ç”ŸæˆåŒ…å«æ‰€æœ‰å†…å®¹çš„æ–°ç´¯ç§¯æ‘˜è¦ï¼ˆä¸è¶…è¿‡{self.summary_max_tokens//10}å­—ï¼‰ï¼š"""
                else:
                    # åˆ›å»ºé¦–ä¸ªæ£€æŸ¥ç‚¹
                    conversation_text = "\n".join([
                        f"{msg.__class__.__name__}: {msg.content}" 
                        for msg in to_summarize[:current_count]
                    ])
                    
                    prompt = f"""è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²æ€»ç»“æˆæ‘˜è¦ï¼ˆä¸è¶…è¿‡{self.summary_max_tokens//10}å­—ï¼‰ï¼š

{conversation_text}"""
                
                response = self.llm.invoke(prompt)
                summary = response.content if hasattr(response, 'content') else str(response)
                
                # ä¿å­˜æ–°æ£€æŸ¥ç‚¹
                self._checkpoints[current_count] = (summary, current_count)
                print(f"ğŸ§  [æ‘˜è¦ç­–ç•¥] åˆ›å»ºæ£€æŸ¥ç‚¹ #{current_count}ï¼ˆç´¯ç§¯æ‘˜è¦ï¼‰")
                
                # ä½¿ç”¨æ–°åˆ›å»ºçš„æ‘˜è¦
                summary_content = summary
                summarized_count = current_count
            else:
                # ä½¿ç”¨æœ€è¿‘çš„æ£€æŸ¥ç‚¹
                if last_checkpoint_count > 0:
                    summary_content, summarized_count = self._checkpoints[last_checkpoint_count]
                    print(f"ğŸ§  [æ‘˜è¦ç­–ç•¥] ä½¿ç”¨æ£€æŸ¥ç‚¹ #{last_checkpoint_count} çš„ç´¯ç§¯æ‘˜è¦")
                else:
                    # æ²¡æœ‰æ£€æŸ¥ç‚¹ï¼Œä¸ä½¿ç”¨æ‘˜è¦
                    return {"llm_input_messages": messages}
            
            # åˆ›å»ºæ‘˜è¦æ¶ˆæ¯
            summary_message = SystemMessage(
                content=f"ã€ç´¯ç§¯å¯¹è¯æ‘˜è¦ï¼ˆå‰{summarized_count}æ¡æ¶ˆæ¯ï¼‰ã€‘\n{summary_content}"
            )
            
            # æ·»åŠ æœªè¢«æ‘˜è¦çš„æ¶ˆæ¯
            unsummarized_messages = to_summarize[summarized_count:] if summarized_count < len(to_summarize) else []
            
            # ç»„åˆæœ€ç»ˆæ¶ˆæ¯
            trimmed_messages = (
                system_messages + 
                [summary_message] + 
                unsummarized_messages + 
                recent_messages
            )
            
            print(f"ğŸ§  [æ‘˜è¦ç­–ç•¥] æœ€ç»ˆæ¶ˆæ¯æ„æˆ: ç³»ç»Ÿ({len(system_messages)}) + æ‘˜è¦(1) + æœªæ‘˜è¦({len(unsummarized_messages)}) + æœ€è¿‘({len(recent_messages)})")
            
            return {"llm_input_messages": trimmed_messages}
        
        return pre_model_hook

class AdaptiveStrategy(BaseMemoryStrategy):
    """è‡ªé€‚åº”ç­–ç•¥ - æ ¹æ®ä¸åŒæƒ…å†µåŠ¨æ€é€‰æ‹©ç­–ç•¥"""
    
    def __init__(self, short_conversation_threshold: int = 10, 
                 long_conversation_max_tokens: int = 2048,
                 token_counter=None):
        self.short_threshold = short_conversation_threshold
        self.max_tokens = long_conversation_max_tokens
        self.token_counter = token_counter or self._default_token_counter
        
    def _default_token_counter(self, messages: List[BaseMessage]) -> int:
        return sum(len(str(msg.content)) // 4 for msg in messages)
        
    def create_pre_model_hook(self) -> Callable:
        def pre_model_hook(state: Dict[str, Any]) -> Dict[str, Any]:
            messages = state.get("messages", [])
            
            # çŸ­å¯¹è¯ï¼šä¿ç•™å…¨éƒ¨
            if len(messages) <= self.short_threshold:
                print(f"ğŸ§  [è‡ªé€‚åº”ç­–ç•¥] çŸ­å¯¹è¯ï¼Œä¿ç•™å…¨éƒ¨ {len(messages)} æ¡æ¶ˆæ¯")
                return {"llm_input_messages": messages}
            
            # é•¿å¯¹è¯ï¼šä½¿ç”¨tokené™åˆ¶
            trimmed_messages = trim_messages(
                messages,
                strategy="last",
                token_counter=self.token_counter,
                max_tokens=self.max_tokens,
                start_on="human",
                end_on=("human", "tool"),
                include_system=True,
            )
            
            print(f"ğŸ§  [è‡ªé€‚åº”ç­–ç•¥] é•¿å¯¹è¯ï¼Œä¿®å‰ª: {len(messages)} -> {len(trimmed_messages)}")
            
            return {"llm_input_messages": trimmed_messages}
        
        return pre_model_hook

# å·¥å‚å‡½æ•°ï¼Œä¾¿äºåˆ›å»ºç­–ç•¥
def create_memory_strategy(strategy_type: str, **kwargs) -> BaseMemoryStrategy:
    """åˆ›å»ºè®°å¿†ç­–ç•¥çš„å·¥å‚å‡½æ•°
    
    Args:
        strategy_type: ç­–ç•¥ç±»å‹ ('sliding_window', 'token_limit', 'summary', 'adaptive')
        **kwargs: ä¼ é€’ç»™ç­–ç•¥æ„é€ å‡½æ•°çš„å‚æ•°
    
    Returns:
        BaseMemoryStrategy: ç­–ç•¥å®ä¾‹
    """
    strategies = {
        'none': NoOpStrategy,
        'sliding_window': SlidingWindowStrategy,
        'token_limit': TokenLimitStrategy,
        'adaptive': AdaptiveStrategy,
        'summary': SummaryStrategy,
    }
    
    if strategy_type not in strategies:
        raise ValueError(f"æœªçŸ¥çš„ç­–ç•¥ç±»å‹: {strategy_type}. å¯é€‰: {list(strategies.keys())}")
    
    return strategies[strategy_type](**kwargs)